{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 2])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "import spatial_vae.models as models\n",
    "####data loading and making coordinates#####\n",
    "mnist_test = np.load('D:/project/CMU/toytask/mnist_rotated/images_test.npy')\n",
    "# img = Image.fromarray(mnist_test[55], 'L')\n",
    "# img.save('my.png')\n",
    "# img.show()\n",
    "mnist_test = torch.from_numpy(mnist_test).float()/255 #normalized\n",
    "n=m=28\n",
    "xgrid = np.linspace(-1, 1, m)\n",
    "#gives a list that contains m=28 evenly spaced values between -1 and 1\n",
    "ygrid = np.linspace(1, -1, n)\n",
    "x0,x1 = np.meshgrid(xgrid, ygrid)\n",
    "#each point in y is paired with a point in x\n",
    "x_coord = np.stack([x0.ravel(), x1.ravel()], 1)# xo.ravel rolls the 2d array x0 \n",
    "x_coord = torch.from_numpy(x_coord).float()#784 points each represented by an x,y pair\n",
    "#tensor of x and y coordinates each corresponding to one of the 784 pixel values\n",
    "print(x_coord.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 list of evenly spaced numbers are created. Each number in 1list is associated with all the other numbers in another list\n",
    "A coordinate frame is created, it has 784 points intended for 784 pixel values. Each point has 2 coordinate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 784])\n"
     ]
    }
   ],
   "source": [
    "y_test = mnist_test.view(-1, n*m)#image pixel values for 10k flattened images\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()#check gpu availability and move data to gpu\n",
    "if use_cuda:\n",
    "    # y_train = y_train.cuda()\n",
    "    y_test = y_test.cuda()\n",
    "    x_coord = x_coord.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_coord is our cartesian frame having 2 coordinates for 784 points per image\n",
    "and y_test is our image data having 784 pixel values per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# training with z-dim: 2\n",
      "# using the spatial generator architecture\n"
     ]
    }
   ],
   "source": [
    "data_test = torch.utils.data.TensorDataset(y_test)\n",
    "#alternative to this would be a customized dataset class \n",
    "import sys\n",
    "import torch.nn as nn\n",
    "#latent value for\n",
    "z_dim = 2\n",
    "print('# training with z-dim:', z_dim, file=sys.stderr)#latent values\n",
    "\n",
    "num_layers = 2\n",
    "hidden_dim = 500\n",
    "activation = nn.Tanh\n",
    "print('# using the spatial generator architecture', file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the model arch z_dim is increased to 5 if rotational and translation inferences are to be done\n",
    "Translation has 2 latent dims associated with it, possibly for accounting it in x0 and x1 directions\n",
    "Rotation has only 1 because angular rotation can happen only in x0 x1 plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# using priors: theta=0.7853981633974483, dx=0.1\n"
     ]
    }
   ],
   "source": [
    "ckpt_path1 = 'D:\\\\project\\\\CMU\\\\toytask\\\\spatial-VAE-master\\\\saved_models_generator_epoch2.sav'\n",
    "ckpt_path2 = 'D:\\\\project\\\\CMU\\\\toytask\\\\spatial-VAE-master\\\\saved_models_inference_epoch2.sav'\n",
    "#load the models\n",
    "#pnet is the spatial vae and qnet is the inference network\n",
    "p_net = torch.load(ckpt_path1)\n",
    "q_net=torch.load(ckpt_path2)\n",
    "#loaded the models including the model class structure etc because it was saved in the original script using torch.save\n",
    "#good practice for saving and loading is to just save and load the state dicts because it doesnt deal with hard coded info\n",
    "#and is flexible\n",
    "if use_cuda:\n",
    "    p_net.cuda()\n",
    "    q_net.cuda()\n",
    "\n",
    "dx_scale = 0.1\n",
    "theta_prior = np.pi/4\n",
    "#prior assumption about the values of rotation and translation\n",
    "print('# using priors: theta={}, dx={}'.format(theta_prior, dx_scale), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "priors are our assumptions about how much rotation or translation is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(mnist_test)\n",
    "\n",
    "params = list(p_net.parameters()) + list(q_net.parameters())\n",
    "lr = 1e-4\n",
    "optim = torch.optim.Adam(params, lr=lr)\n",
    "#define optimizer\n",
    "minibatch_size = 100\n",
    "\n",
    "test_iterator = torch.utils.data.DataLoader(data_test, batch_size=minibatch_size)\n",
    "#data loader for dynamic loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be read in the order 1. eval_model 2. eval_minibatch. In eval_minibatch, we first use the inference network to get the latent distribution of the image using its higher representations. Then we do rotation and translation operations on our pre fixed coordinate/cartesian system by using samples from the inferenced latent distribution as the other operand in respective tasks. This establishes a relationship of dependence between transformed cartesian coordinates and latent distribution, which in my opinion explicitly reflects changes in one into the other. Using the transformed coordinates we reconstruct our image rotated and translated using the spatial vae model. The spatial vae model has learned a function that directly takes coordinate values and latent values to form the desired image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_minibatch(x, y, p_net, q_net, rotate=True, translate=True, dx_scale=0.1, theta_prior=np.pi, use_cuda=False):\n",
    "    #y is a batch of images , x is the set of coordinates in cartesian system\n",
    "    b = y.size(0)#batch size\n",
    "    x = x.expand(b, x.size(0), x.size(1))#784pixels,2coords for each image in a batch of 100\n",
    "    #does some replication of values to give a new view of tensor although the original tensor remains same\n",
    "    # first do inference on the latent variables\n",
    "    if use_cuda:\n",
    "        y = y.cuda()\n",
    "\n",
    "    z_mu,z_logstd = q_net(y)\n",
    "    #get an estimate about the mean and std dev, and thus distriution,of latent vars, establishes a relation between latent variables and images \n",
    "    z_std = torch.exp(z_logstd) #exponential the log\n",
    "    z_dim = z_mu.size(1)\n",
    "    #z=5, 2 are unstructured,1 for translational, 2 for rotational ??idk why maybe they require 2 for mean n std\n",
    "\n",
    "    # draw samples from variational posterior to calculate\n",
    "    # E[p(x|z)]   \n",
    "    r = Variable(x.data.new(b,z_dim).normal_())#find the difference between x.new and x.data.new \n",
    "    #creates an autograd tensor from a gaussian distr with mean 0 and std 1 havin the same data type as X and\n",
    "    #shape as X's view in 100,5\n",
    "    # 100,5 tensor initialized with normal distribution; autograd tensor\n",
    "    #each point his sampled from this random distribution and projected to the latent distribution\n",
    "    z = z_std*r + z_mu \n",
    "    #latent vector\n",
    "    kl_div = 0\n",
    "    if rotate:\n",
    "        # z[0] is the rotation\n",
    "        theta_mu = z_mu[:,0]#theta_mu and other theta variables are added to the computational graph\n",
    "        theta_std = z_std[:,0]\n",
    "        theta_logstd = z_logstd[:,0]\n",
    "        theta = z[:,0]\n",
    "        z = z[:,1:]#isolate the latent variable of rotation from others\n",
    "        z_mu = z_mu[:,1:]\n",
    "        z_std = z_std[:,1:]\n",
    "        z_logstd = z_logstd[:,1:]\n",
    "\n",
    "        # calculate rotation matrix\n",
    "        rot = Variable(theta.data.new(b,2,2).zero_())#get zero tensors having same type as theta and size as mentioned\n",
    "        rot[:,0,0] = torch.cos(theta)\n",
    "        rot[:,0,1] = torch.sin(theta)\n",
    "        rot[:,1,0] = -torch.sin(theta)\n",
    "        rot[:,1,1] = torch.cos(theta)\n",
    "        #to do the coordinate transformation by rotation\n",
    "        x = torch.bmm(x, rot) # rotate coordinates by theta\n",
    "\n",
    "        # calculate the KL divergence term\n",
    "        sigma = theta_prior\n",
    "        #loss term for inference on latent\n",
    "        kl_div = -theta_logstd + np.log(sigma) + (theta_std**2 + theta_mu**2)/2/sigma**2 - 0.5\n",
    "\n",
    "    if translate:\n",
    "        # z[0,1] are the translations\n",
    "        dx_mu = z_mu[:,:2]\n",
    "        dx_std = z_std[:,:2]\n",
    "        dx_logstd = z_logstd[:,:2]\n",
    "        dx = z[:,:2]*dx_scale # scale dx by standard deviation\n",
    "        dx = dx.unsqueeze(1)\n",
    "        z = z[:,2:]\n",
    "\n",
    "        x = x + dx # translate coordinates\n",
    "    # reconstruct the image by making it depend on 784 rotated and translated cartesian coordinates + latent variables\n",
    "    #784coords+5latent_vars input gives 784 pixel values\n",
    "    y_hat = p_net(x.contiguous(), z)\n",
    "    y_hat = y_hat.view(b, -1)\n",
    "#     y_hat1 = y_hat.reshape(b,28,28).cpu().detach().numpy()\n",
    "#     img = Image.fromarray(y_hat1[55], 'L')\n",
    "#     img.save('my.png')\n",
    "#     img.show()\n",
    "    size = y.size(1)\n",
    "    log_p_x_g_z = -F.binary_cross_entropy_with_logits(y_hat, y)*size\n",
    "    z_kl = -z_logstd + 0.5*z_std**2 + 0.5*z_mu**2 - 0.5\n",
    "    kl_div = kl_div + torch.sum(z_kl, 1)\n",
    "    kl_div = kl_div.mean()\n",
    "    \n",
    "    elbo = log_p_x_g_z - kl_div\n",
    "\n",
    "    return elbo, log_p_x_g_z, kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(iterator, x_coord, p_net, q_net, rotate=True, translate=True\n",
    "              , dx_scale=0.1, theta_prior=np.pi, use_cuda=False):\n",
    "    p_net.eval()\n",
    "    q_net.eval()\n",
    "    #iterator batch of 10k images 784 pixels each, x_coord is cartesian system 784 points with xandy cords\n",
    "    c = 0\n",
    "    gen_loss_accum = 0\n",
    "    kl_loss_accum = 0\n",
    "    elbo_accum = 0\n",
    "\n",
    "    for y, in iterator:\n",
    "        b = y.size(0)\n",
    "        x = Variable(x_coord)# 784 points with 2 coordinates each \n",
    "        y = Variable(y) #batchsize,100 images 784 pixel values each\n",
    "#         print(x.shape,y.shape)\n",
    "        elbo, log_p_x_g_z, kl_div = eval_minibatch(x, y, p_net, q_net, rotate=rotate, translate=translate\n",
    "                                                  , dx_scale=dx_scale, theta_prior=theta_prior\n",
    "                                                  , use_cuda=use_cuda)\n",
    "\n",
    "        elbo = elbo.item() #detaches tensors/losses from the computational graph so that they dont burden the computational processes \n",
    "        gen_loss = -log_p_x_g_z.item()\n",
    "        kl_loss = kl_div.item()\n",
    "\n",
    "        c += b\n",
    "        delta = b*(gen_loss - gen_loss_accum)\n",
    "        gen_loss_accum += delta/c\n",
    "\n",
    "        delta = b*(elbo - elbo_accum)\n",
    "        elbo_accum += delta/c\n",
    "\n",
    "        delta = b*(kl_loss - kl_loss_accum)\n",
    "        kl_loss_accum += delta/c\n",
    "\n",
    "    return elbo_accum, gen_loss_accum, kl_loss_accum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tSplit\tELBO\tError\tKL\n",
      "test\t-215.1652920532226\t206.2328868103028\t8.932405633926392\n"
     ]
    }
   ],
   "source": [
    "output = sys.stdout\n",
    "print('\\t'.join(['Epoch', 'Split', 'ELBO', 'Error', 'KL']), file=output)\n",
    "\n",
    "elbo_accum,gen_loss_accum,kl_loss_accum = eval_model(test_iterator, x_coord, p_net,q_net, rotate=True, translate=True,dx_scale=dx_scale, theta_prior=theta_prior,use_cuda=use_cuda)\n",
    "line = '\\t'.join(['test', str(elbo_accum), str(gen_loss_accum), str(kl_loss_accum)])\n",
    "print(line, file=output)\n",
    "output.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
